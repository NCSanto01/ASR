## EXTRA 1. Escalado vertical de pods con VPA

VPA (o autoescalado vertical de pods) nos libera de la responsabilidad
de tener que conocer perfectamente los valores de CPU que 
tenemos que seleccionar para cada contenedor. De hecho, el auto-escalador
VPA nos recomienda valores de CPU y RAM, y l칤mites, acorde
al hist칩rico de uso. Adem치s, evidentemente, puede hacer que
susodichos valores sean los que se utilicen en los nodos del
despliegue.

游뚿 *Atenci칩n*: No deber칤amos usar nunca un VPA y un HPA simult치neamente
sobre la misma m칠trica (CPU o memoria). La raz칩n es sencilla, si
as칤 fuera, ambos auto-escaladores intentar칤an responder al cambio
de demanda en la misma m칠trica y llevar칤a a un conflicto de 
responsabilidades. No obstante, s칤 que se pueden usar (y se
recomienda de hecho) en distintas, por ejemplo VPA en CPU o memoria
y el HPA en m칠tricas personalizadas para evitar el solapamiento.


Si recuerdas, cuando hemos creado el cluster, hemos seleccionado
la opci칩n de autoescalado vertical. Es por ello que VPA ya est치
activo en nuestro cluster. Para comprobarlo, solo tenemos que
ejecutar el siguiente comando:

```shell
$ gcloud container clusters describe scaling-demo | grep ^verticalPodAutoscaling -A 1
```

Este comando nos deber칤a devolver en terminal: `enabled: true`,
confirmando que est치 activo. Si por lo que fuera se nos olvid칩 
activarlo con la creaci칩n del cluster, podemos hacerlo ahora de 
forma manual:

```shell
$ gcloud container clusters update $cluster_name \
    --enable-vertical-pod-autoscaling
```

Para comprobar las virtudes de VPA, vamos a desplegar un nuevo
servicio, `hello-server`:

```shell
$ kubectl create deployment hello-server \
    --image=gcr.io/google-samples/hello-app:1.0
```

Esto llevar치 unos minutos. Cuando acabe el despliegue, 
podremos asegurarnos de que todo ha ido bien mediante el
siguiente comando:

```shell
$ kubectl get deployment hello-server
```

A continuaci칩n, vamos a definir la m칤nima cantidad de CPU
necesaria por el despliegue `hello-server`. En el lenguaje
de K8s esto se conoce como *resource requests* (RR). Por ejemplo,
vamos a comenzar seleccionando un RR de 450 mili-CPU, i.e. 0.45 CPUs.
Esto se lleva a cabo mediante:

```shell
$ kubectl set resources deployment hello-server \
  --requests=cpu=450m
```

Ahora podemos inspeccionar el pod que sirve `hello-server`
y ver que la configuraci칩n de `Requests` es precisamente
la que acabamos de configurar:

```shell
$ kubectl describe pod hello-server | sed -n "/Containers:$/,/Conditions:/p"
```

En lo que sigue, vamos a usar el manifiesto de configuraci칩n del
VPA, [hello-vpa.yaml](hello-vpa.yaml):

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: hello-server-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       hello-server
  updatePolicy:
    updateMode: "Off"
```

Este manifiesto generar치 un auto-esclador VPA para el 
despliegue `hello-server` con una pol칤tica de actualizaci칩n 
`Off`. Un VPA solo puede tener una de las siguientes tres
pol칤ticas de actualizaci칩n:

- *Off*: Con esta configuraci칩n VPA solo generar치 recomendaciones
  basadas en datos hist칩ricos. Dichas recomendaciones no ser치n
  aplicadas, las tendremos que aplicar nosotros manualmente
- *Initial*: VPA generar치 recomendaciones y crear치 nuevos pods
  basadas en las mismas una sola vez, no cambiar치 el tama침o de los
  pods a posteriori
- *Auto*: VPA borrar치 y crear치 regularmente pods para que se cumplan
  las recomendaciones actualizadas regularmente
  
Conocido esto, apliquemos el manifiesto mencionado:

```shell
$ kubectl apply -f hello-vpa.yaml
```

Tras esperar un minuto, aproximadamente, podemos hacer una comprobaci칩n
del VPA mediante:

```shell
$ kubectl describe vpa hello-server-vpa
```

Dado que nuestro manifiesto especificaba una pol칤tica de 
actualizaci칩n modo `Off`, lo que podremos ver ser치n las recomendaciones
generadas por la VPA. Para ello, podemos fijarnos en la secci칩n
`Container Recommendations` que deber칤a aparecernos al final de
la respuesta del anterior comando. Ah칤 veremos diferentes
tipos de recomendaci칩n, cada uno de ellos con valores de CPU
y memoria. Si todo ha ido bien, veremos que el VPA nos est치 
recomendando que bajemos la CPU RR a `25m`, en lugar del valor
previo, adem치s de darnos un valor de cuanta memoria deber칤a
requerirse. A partir de aqu칤, podr칤amos aplicar estas recomendaciones
manualmente. 

丘멆잺 Las recomendaciones dadas por el VPA vienen dadas en base
a los datos recolectados, por lo que para 칠stas sean lo m치s
칰tiles posibles, deber칤amos esperar a recolectar aproximadamente 
unas 24h si estamos en el modo `Off`.

En lugar de aplicar las recomendaciones manualmente, vamos a 
proceder a cambiar la pol칤tica de actualizaci칩n del VPA.
Para ello ya tenemos preparado el manifiesto modificado
en [hello-vpa-auto.yaml](hello-vpa-auto.yaml):

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: hello-server-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       hello-server
  updatePolicy:
    updateMode: "Auto"
```

Lo 칰nico que tenemos que hacer es actualizar el VPA
con el manifiesto mencionado mediante el siguiente comando:

```shell
$ kubectl apply -f hello-vpa-auto.yaml
```

Para que el VPA pueda adaptar el tama침o del pod, 
primero necesitar치 borra el pod y posteriormente recrearlo
con la nueva configuraci칩n de tama침o. 
Por configuraci칩n de defecto, VPA no va a borrar el 
칰ltimo pod activo, que es el que estamos estudiando. As칤
que necesitaremos al menos 2 r칠plicas para ver como el VPA
hace los cambios adecuados. Para ello, vamos a escalar
nosotros manualmente ahora el despliegue `hello-server` 
a dos r칠plicas:

```shell
$ kubectl scale deployment hello-server --replicas=2
```

A continuaci칩n, podemos monitorizar el comportamiento de los
pods mediante:

```shell
$ kubectl get pods -w
```

Esperamos hasta que veamos los pods `hello-server-xxx` en
el estado `terminating`. Esta es la se침al de que nuestra VPA
ya est치 borrando y reconfigurando el tama침o de los pods.
Una vez que lo veamos podremos simplemente salir de la espera
pulsando `Ctrl + c` en nuestro terminal.


### 1.1. Comprobaci칩n de los autoescaladores VPA


Pasado unos minutos, el auto-escalador VPA habr치 ya entrado
en juego y re-escalado los pods. Para ver si es as칤 podemos
comprobarlo mediante:

```shell
$ kubectl describe pod hello-server | sed -n "/Containers:$/,/Conditions:/p"
```

Buscando el campo `Requests`, deber칤amos ver ahora un valor inferior en 
cuanto a CPU se refiere, lo que significar칤a que el VPA ha entrado
en acci칩n y ha hecho lo que deb칤a (dada la falta de actividad).



## EXTRA 2. Auto aprovisionamiento de nodos

Los clusters de GKE de Google tienen una opci칩n que es autoaprovionamiento de nodos.

##### Node auto-provisioning creates node pools based on the following information:
- CPU, memory and ephemeral storage resource requests.
- GPU requests
- Pending Pods' node affinities and label selectors.
- Pending Pods' node taints and tolerations.

El autoescalado NAP consiste en a침adir nuevos nodos al pool
de nodos asociado al cluster, pero con el tama침o adecuado para
adaptarse a la demanda. En ausencia de NAP, el autoecalador
de cluster crear칤a nodos con las mismas caracter칤sticas
de los ya existentes, no adapt치ndose as칤 verticalmente a la
demanda. Es por ello que el NAP es tan conveniente para un
uso adecuado de los recursos, m치s a칰n cuando tenemos cargas
de trabajo que son secuenciales (por *batches*), ya que 
con este modo de escalamiento el pool est치 optimizado para
nuestro caso de uso espec칤fico.

Para activar NAP en nuestro cluster:

```shell
gcloud container clusters update $cluster_name \
    --enable-autoprovisioning \
    --min-cpu 1 \
    --min-memory 2 \
    --max-cpu 45 \
    --max-memory 160
```

Donde estamos especificando el m칤nimo y m치ximo n칰mero de recursos
de CPU y memoria. Recordemos que esta estrategia es aplicable
al cluster completo. El NAP puede tardar unos minutos en activarse,
y a pesar de ello, puede ser que en nuestro ejemplo no entre en
juego dado el estado actual de nuestro cluster.


